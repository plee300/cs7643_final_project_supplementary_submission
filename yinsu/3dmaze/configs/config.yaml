# # configs/config.yaml

# # --- Algorithm Selection ---
# # The main switch to choose which algorithm to run. Options: "PPO" or "DQN"
# algorithm_to_run: "DQN"

# # --- Environment Settings ---
# env:
#   name: "MemoryMaze-9x9-v0"
#   wrappers:
#     grayscale: {enabled: true}
#     resize: {enabled: true, shape: [84, 84]}
#     frame_stack: {enabled: true, k: 4}

# # --- Model Architecture Settings ---
# model:
#   recurrent_hidden_dim: 256 # Used only by PPO's recurrent model

# # --- PPO Algorithm Hyperparameters ---
# ppo:
#   learning_rate: 0.00025
#   gamma: 0.99
#   use_gae: true
#   gae_lambda: 0.95
#   clip_epsilon: 0.1
#   ppo_epochs: 4
#   num_mini_batches: 4
#   value_loss_coef: 0.5
#   entropy_coef: 0.01
#   max_grad_norm: 0.5
#   # PPO-specific rollout collection settings
#   num_processes: 8
#   num_steps_per_update: 128

# # --- DQN Algorithm Hyperparameters ---
# dqn:
#   dueling: true            # Enable/Disable the Dueling DQN architecture
#   double: true             # Enable/Disable the Double DQN learning update
#   learning_rate: 0.0001
#   gamma: 0.99
#   buffer_size: 100000      # Size of the replay buffer
#   batch_size: 32           # Mini-batch size for sampling from the buffer
#   target_update_frequency: 1000 # How often to update the target network (in steps)
#   learning_starts: 10000   # Number of steps to collect before starting to learn
#   # Epsilon-greedy exploration settings
#   epsilon_start: 1.0
#   epsilon_end: 0.01
#   epsilon_decay_steps: 200000 # How many steps to decay epsilon over

# # --- General Training Settings ---
# training:
#   device: "auto"
#   num_env_steps: 10000000
#   output_dir: "experiment_results/" # All runs will be saved in subfolders here
#   logger_type: "tensorboard" # Options: "tensorboard" or "console"
#   # Note: PPO logs per update, DQN logs per step. These are for DQN.
#   log_interval_steps: 10000
#   save_interval_steps: 100000

# # --- Live Visualization Settings (during training) ---
# live_visualization:
#   enabled: false              # Set to true to enable periodic rendering
#   eval_interval_steps: 50000 # Render one episode every N training steps

# # --- Evaluation Settings ---
# evaluation:
#   record_video: true          # Set to true to save a video of the agent's performance
#   video_folder: "videos"      # A subfolder inside the experiment directory to save videos
#   num_episodes: 10            # How many episodes to run during evaluation

# configs/config.yaml

# --- Algorithm Selection ---
# The main switch to choose which algorithm to run. Options: "PPO" or "DQN"
algorithm_to_run: "DQN"

# --- Environment Settings ---
env:
  name: "MemoryMaze-9x9-v0"
  wrappers:
    grayscale: {enabled: true}
    resize: {enabled: true, shape: [84, 84]}
    frame_stack: {enabled: true, k: 4}

# --- Model Architecture Settings ---
model:
  recurrent_hidden_dim: 256 # Used only by PPO's recurrent model

# --- PPO Algorithm Hyperparameters ---
ppo:
  learning_rate: 0.00025
  gamma: 0.99
  use_gae: true
  gae_lambda: 0.95
  clip_epsilon: 0.1
  ppo_epochs: 4
  num_mini_batches: 4
  value_loss_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  # PPO-specific rollout collection settings
  num_processes: 8
  num_steps_per_update: 128

# --- DQN Algorithm Hyperparameters ---
dqn:
  dueling: true              # Enable/Disable the Dueling DQN architecture
  double: true               # Enable/Disable the Double DQN learning update
  learning_rate: 0.0001
  gamma: 0.99
  buffer_size: 100000        # Size of the replay buffer
  batch_size: 32             # Mini-batch size for sampling from the buffer
  train_frequency: 4         # Train every N steps (how often to update the network)
  target_update_frequency: 1000  # How often to update the target network (in steps)
  learning_starts: 10000     # Number of steps to collect before starting to learn
  # Epsilon-greedy exploration settings
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay_steps: 200000  # How many steps to decay epsilon over

# --- General Training Settings ---
training:
  device: "auto"             # "auto" for automatic GPU/CPU selection, or "cuda"/"cpu"
  seed: 42                   # Random seed for reproducibility
  num_env_steps: 10000000    # Total number of environment steps to train
  output_dir: "experiment_results/"  # All runs will be saved in subfolders here
  logger_type: "tensorboard" # Options: "tensorboard" or "console"
  # Note: PPO logs per update, DQN logs per step. These are for DQN.
  log_interval_steps: 10000
  save_interval_steps: 100000

# --- Live Visualization Settings (during training) ---
live_visualization:
  enabled: false             # Set to true to enable periodic rendering
  eval_interval_steps: 50000 # Render one episode every N training steps

# --- Evaluation Settings ---
evaluation:
  record_video: true         # Set to true to save a video of the agent's performance
  video_folder: "videos"     # A subfolder inside the experiment directory to save videos
  num_episodes: 10           # How many episodes to run during evaluation